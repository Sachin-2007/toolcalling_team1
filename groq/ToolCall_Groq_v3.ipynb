{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***RAHUL VIMALKANTH***"
      ],
      "metadata": {
        "id": "c08nWZbLg2Cb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORTING LIBRARIES AND INPUT FILES"
      ],
      "metadata": {
        "id": "WOHAhRLgf7Da"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7wXOlKk8hxt9"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "%pip -qq install groq\n",
        "%pip -qq install nltk rouge-score\n",
        "%pip install gradio -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required modules\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from groq import Groq\n",
        "import gradio as gr\n",
        "import json\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer"
      ],
      "metadata": {
        "id": "vlVPJePGi0gj"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Groq client with API key from user data\n",
        "client = Groq(api_key=userdata.get('GROQ_API_KEY'))\n",
        "MODEL1 = \"llama3-groq-70b-8192-tool-use-preview\"\n",
        "MODEL2 = \"llama3-groq-70b-8192-tool-use-preview\""
      ],
      "metadata": {
        "id": "Klm6cEPhjWb6"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The *tools.json* contains the tools provided in the problem statement. The *examples.json* file has the examples provided in the problem statement. The *generated_data_examples.json* contains high quality queries and solutions that I generated using another LLM."
      ],
      "metadata": {
        "id": "mDmj461-e-98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load JSON data from a file\n",
        "def load_json_file(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            return json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "tools = load_json_file(\"tools.json\")\n",
        "examples = load_json_file(\"examples.json\")\n",
        "generated_data_examples = load_json_file(\"generated_data_examples.json\")"
      ],
      "metadata": {
        "id": "fb9XWaqrc0cV"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CREATING PROMPT TEMPLATE FOR THE MODEL"
      ],
      "metadata": {
        "id": "x8aF4Bd1gKZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_format = {\n",
        "\"type\": \"array\",\n",
        "\"items\": {\n",
        "\"type\": \"object\",\n",
        "\"properties\": {\n",
        "\"tool_name\": {\n",
        "\"type\": \"string\"\n",
        "},\n",
        "\"arguments\": {\n",
        "\"type\": \"array\",\n",
        "\"items\": {\n",
        "\"type\": \"object\",\n",
        "\"properties\": {\n",
        "\"argument_name\": {\n",
        "\"type\": \"string\"\n",
        "},\n",
        "\"argument_value\": {\n",
        "\"type\": [\"string\", \"array\", \"object\", \"boolean\", \"number\"]\n",
        "}\n",
        "},\n",
        "\"required\": [\n",
        "\"argument_name\",\n",
        "\"argument_value\"\n",
        "]\n",
        "}\n",
        "}\n",
        "},\n",
        "\"required\": [\n",
        "\"tool_name\",\n",
        "\"arguments\"\n",
        "]\n",
        "}\n",
        "}"
      ],
      "metadata": {
        "id": "jUkakMBic80e"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create a prompt for MODEL1\n",
        "def prompt_template_model1(query, tools, output_format, examples):\n",
        "    return f\"\"\"\n",
        "<|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are a query solver. Using the provided tools, your task is to solve the query by selecting and calling the relevant tools in sequence.\n",
        "\n",
        "### Key Instructions:\n",
        "1) Carefully read the tool and argument descriptions to understand what outputs a tool generates and what inputs it requires.\n",
        "2) Use the output of previous tool calls as input for the current tool, referenced as $$PREV[i], where `i` is the index of the previous tool call.\n",
        "3) Always check if authentication tools like \"who_am_i\", \"team_id\", or \"get_sprint_id\" are necessary. Do not use more than 4 tools.\n",
        "4) Handle the \"type\" argument in \"works_list\" correctly (e.g., \"issue\", \"ticket\", or \"task\").\n",
        "5) Stop once the task is complete, and no further tools are needed to solve the query.\n",
        "6) Only use the tools provided. If the query cannot be solved with the given tools, return an empty list: [].\n",
        "7) Understand the solutions for the given queries in the examples provided.\n",
        "\n",
        "### Available Tools:\n",
        "<tools>\n",
        "{tools}\n",
        "</tools>\n",
        "\n",
        "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Analyze the user query and select only the relevant tools. Reference the output of previous tools using $$PREV[i], and generate a sequence of tool calls that will solve the query.Follow the JSON schema provided in <output_format></output_format>.\n",
        "\n",
        "If any tool has an empty arguments list, do not include it. Return only the JSON output without explanations. If the query cannot be solved with the available tools, return an empty list: [].\n",
        "\n",
        "### JSON Output Format.Again strictly follow the given output format:\n",
        "<output_format>\n",
        "{output_format}\n",
        "</output_format><|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "### Examples with queries and their corresponging tool calls.\n",
        "{examples}\n",
        "<|eot_id|><|start_header_id|>tool<|end_header_id|>\n",
        "    \"\"\".strip()\n"
      ],
      "metadata": {
        "id": "MGKoNn0hleYH"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create a prompt for MODEL2\n",
        "def prompt_template_model2(query, model1_output, generated_data_examples, output_format):\n",
        "    \"\"\"Create a concise and efficient prompt for MODEL2 to refine the output of MODEL1.\"\"\"\n",
        "    return f\"\"\"\n",
        "<|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are an advanced AI tasked with refining the tool calls generated by a previous model (MODEL1). Your goal is to optimize the output to directly address the user's query using only the necessary tools and accurate arguments.\n",
        "\n",
        "### Your Task:\n",
        "1. **User Query**: Solve this query using the provided tools.\n",
        "2. **MODEL1 Output**: Review and correct the tool calls generated by MODEL1, improving them as needed.\n",
        "3. **Examples**: Refer to these examples for guidance on how to correct tool calls and arguments.\n",
        "4. **Generated Examples**: Search these examples for queries which are similar to user query. Understand the solutions for those queries and try to answer the user query.\n",
        "\n",
        "### Requirements:\n",
        "- Only include tools that are strictly relevant to solving the query.\n",
        "- All arguments must be accurately filled and non-empty.\n",
        "- Remove any unnecessary, redundant, or incorrect tool calls.\n",
        "- Ensure output is valid JSON, strictly following the format below.\n",
        "\n",
        "### Instructions:\n",
        "1. **Optimize Tool Selection**: Only include tools that are essential. Omit any irrelevant tool calls.\n",
        "2. **Ensure Argument Accuracy**: Arguments should be filled based on the query, referring to `$$PREV[i]` for dependencies from prior tools.\n",
        "3. **JSON Format**: Output must adhere strictly to the following JSON format within the <output_format> tags:\n",
        "\n",
        "### JSON Output Format:\n",
        "<output_format>\n",
        "{output_format}\n",
        "</output_format>\n",
        "\n",
        "### Input:\n",
        "- **User Query**:\n",
        "  {query}\n",
        "- **MODEL1 Output**:\n",
        "  {model1_output}\n",
        "- **Generated Examples**:\n",
        "  {generated_data_examples}\n",
        "\n",
        "Provide your refined tool calls in the specified JSON format, ensuring no unnecessary tools or empty arguments are included.\n",
        "\n",
        "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "\n",
        "\n",
        "\"\"\".strip()\n"
      ],
      "metadata": {
        "id": "QGhkoY-9qZMP"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PROMPTING MODEL1 AND MODEL2"
      ],
      "metadata": {
        "id": "eyHr_4fdgU4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"List all high severity tickets coming in from slack from customer Cust123 and generate a summary of them.\"\n",
        "# Execute the queries and print the results\n",
        "\n",
        "prompt1 = prompt_template_model1(query, tools, output_format,examples)\n",
        "\n",
        "# Call MODEL1 to generate the initial tool calls\n",
        "chat_completion1 = client.chat.completions.create(\n",
        "    model=MODEL1,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": prompt1},\n",
        "        {\"role\": \"user\", \"content\": query},\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Capture the response from MODEL1\n",
        "model1_response = chat_completion1.choices[0].message.content\n",
        "print(\"MODEL1 Output:\", model1_response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "905LwPGj8B2t",
        "outputId": "e8a11a6d-bb6f-42f8-ecfa-78a7af5eecc6"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MODEL1 Output: [{'tool_name': 'search_object_by_name', 'arguments': [{'argument_name': 'query', 'argument_value': 'Cust123'}]}, {'tool_name': 'works_list', 'arguments': [{'argument_name': 'ticket_severity', 'argument_value': ['high']}, {'argument_name': 'ticket_source_channel', 'argument_value': ['slack']}, {'argument_name': 'ticket_rev_org', 'argument_value': '$$PREV[0]'}, {'argument_name': 'type', 'argument_value': ['ticket']}]}, {'tool_name': 'summarize_objects', 'arguments': [{'argument_name': 'objects', 'argument_value': '$$PREV[0]'}]}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate prompt for MODEL2\n",
        "prompt2 = prompt_template_model2(query, model1_response, generated_data_examples, output_format)\n",
        "\n",
        "# Call MODEL2 to refine the output using MODEL1's response and the examples\n",
        "chat_completion2 = client.chat.completions.create(\n",
        "  model=MODEL2,\n",
        "  messages=[\n",
        "      {\"role\": \"system\", \"content\": prompt2},\n",
        "      {\"role\": \"user\", \"content\": query},\n",
        "  ]\n",
        ")\n",
        "\n",
        "# Capture the response from MODEL2\n",
        "model2_response = chat_completion2.choices[0].message.content\n",
        "print(\"MODEL2 Refined Output:\", model2_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyG22y7Lj2WI",
        "outputId": "8f465961-a448-400a-dc96-1b2d5e85a088"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MODEL2 Refined Output: {'id': 1, 'name': 'search_object_by_name', 'arguments': {'query': 'Cust123'}}\n",
            "{'id': 2, 'name': 'works_list', 'arguments': {'ticket_severity': ['high'], 'ticket_source_channel': ['slack'], 'ticket_rev_org': '$$PREV[0]', 'type': ['ticket']}}\n",
            "{'id': 3, 'name': 'summarize_objects', 'arguments': {'objects': '$$PREV[1]'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "actual_response =[\n",
        "          {\n",
        "            \"tool_name\": \"search_object_by_name\",\n",
        "            \"arguments\": [\n",
        "              {\n",
        "                \"argument_name\": \"query\",\n",
        "                \"argument_value\": \"Cust123\"\n",
        "              }\n",
        "            ]\n",
        "          },\n",
        "          {\n",
        "            \"tool_name\": \"works_list\",\n",
        "            \"arguments\": [\n",
        "              {\n",
        "                \"argument_name\": \"ticket_rev_org\",\n",
        "                \"argument_value\": [\n",
        "                  \"$$PREV[0]\"\n",
        "                ]\n",
        "              },\n",
        "              {\n",
        "                \"argument_name\": \"ticket_severity\",\n",
        "                \"argument_value\": [\n",
        "                  \"high\"\n",
        "                ]\n",
        "              },\n",
        "              {\n",
        "                \"argument_name\": \"ticket_source_channel\",\n",
        "                \"argument_value\": [\n",
        "                  \"slack\"\n",
        "                ]\n",
        "              },\n",
        "              {\n",
        "                \"argument_name\": \"type\",\n",
        "                \"argument_value\": [\n",
        "                  \"ticket\"\n",
        "                ]\n",
        "              }\n",
        "            ]\n",
        "          },\n",
        "          {\n",
        "            \"tool_name\": \"summarize_objects\",\n",
        "            \"arguments\": [\n",
        "              {\n",
        "                \"argument_name\": \"objects\",\n",
        "                \"argument_value\": \"$$PREV[1]\"\n",
        "              }\n",
        "            ]\n",
        "          }\n",
        "        ]"
      ],
      "metadata": {
        "id": "hDiKqSSKQaMI"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EVALUATING THE MODEL PERFORMANCE"
      ],
      "metadata": {
        "id": "zKKCMabogoyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate ROUGE score\n",
        "def calculate_rouge(reference, candidate):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "    scores = scorer.score(reference, candidate)\n",
        "\n",
        "    return scores\n",
        "\n",
        "# Evaluate MODEL1's response against ground truth (example response)\n",
        "rouge_scores_model1 = calculate_rouge(json.dumps(actual_response), json.dumps(model1_response))\n",
        "rouge_scores_model2 = calculate_rouge(json.dumps(actual_response), json.dumps(model2_response))\n",
        "\n",
        "# Print the ROUGE scores\n",
        "print(f\"ROUGE Scores for MODEL1: {rouge_scores_model1}\")\n",
        "\n",
        "print(f\"ROUGE Scores for MODEL2: {rouge_scores_model2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIjxkCFoRC6o",
        "outputId": "c8535302-528c-4735-ec8f-d4ac68554f2b"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROUGE Scores for MODEL1: {'rouge1': Score(precision=0.9814814814814815, recall=0.8833333333333333, fmeasure=0.9298245614035088), 'rougeL': Score(precision=0.8888888888888888, recall=0.8, fmeasure=0.8421052631578948)}\n",
            "ROUGE Scores for MODEL2: {'rouge1': Score(precision=0.7948717948717948, recall=0.5166666666666667, fmeasure=0.6262626262626263), 'rougeL': Score(precision=0.6923076923076923, recall=0.45, fmeasure=0.5454545454545455)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UI USING GRADIO"
      ],
      "metadata": {
        "id": "ghTmg0W9gsVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(query):\n",
        "\n",
        "    # Assume these functions are defined with your existing logic for MODEL1 and MODEL2 processing\n",
        "    prompt1 = prompt_template_model1(query, tools, output_format, examples)\n",
        "    chat_completion1 = client.chat.completions.create(\n",
        "        model=MODEL1,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": prompt1},\n",
        "            {\"role\": \"user\", \"content\": query},\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    model1_response = chat_completion1.choices[0].message.content\n",
        "\n",
        "    prompt2 = prompt_template_model2(query, model1_response, generated_data_examples, output_format)\n",
        "    chat_completion2 = client.chat.completions.create(\n",
        "        model=MODEL2,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": prompt2},\n",
        "            {\"role\": \"user\", \"content\": query},\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    model2_response = chat_completion2.choices[0].message.content\n",
        "\n",
        "    return model1_response, model2_response\n"
      ],
      "metadata": {
        "id": "uAdZ8g41MJeu"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks(theme=\"dark\", css=\"\"\"\n",
        "    .gr-button {background-color: #4CAF50; color: white; border: none;}\n",
        "    .gr-button:hover {background-color: #45a049;}\n",
        "    .gr-textbox {background-color: #333; color: white; border-color: #4CAF50;}\n",
        "\"\"\") as ui:\n",
        "\n",
        "    # Main Header\n",
        "    gr.Markdown(\"<h1 style='color:#4CAF50; text-align:center;'>Tool Use Automation Interface</h1>\")\n",
        "\n",
        "    # Input Section\n",
        "    with gr.Row():\n",
        "        query_input = gr.Textbox(label=\"Enter your query\", placeholder=\"Input your query here...\", lines=2)\n",
        "\n",
        "    # Submit Button\n",
        "    submit_button = gr.Button(\"Submit\", elem_id=\"submit_button\")\n",
        "\n",
        "    # Outputs for model responses\n",
        "    with gr.Row():\n",
        "        model1_output = gr.Textbox(label=\"MODEL1 Output\", placeholder=\"Model1 output will appear here\", lines=4)\n",
        "        model2_output = gr.Textbox(label=\"MODEL2 Refined Output\", placeholder=\"Model2 refined output will appear here\", lines=4)\n",
        "\n",
        "    # Submit action to trigger model generation\n",
        "    submit_button.click(fn=generate_response, inputs=[query_input], outputs=[model1_output, model2_output])\n",
        "\n",
        "# Launch the interface\n",
        "ui.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "zMBVw4icJCXm",
        "outputId": "9b511bef-b724-41db-e9f9-3d590548d916"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/blocks.py:1025: UserWarning: Cannot load dark. Caught Exception: 404 Client Error: Not Found for url: https://huggingface.co/api/spaces/dark (Request ID: Root=1-67129284-472ce72d46fed8da59d903a2;8eb95bc2-a270-497d-993b-ea1653c5b421)\n",
            "\n",
            "Sorry, we can't find the page you are looking for.\n",
            "  warnings.warn(f\"Cannot load {theme}. Caught Exception: {str(e)}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://e542e6fad7561282bb.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e542e6fad7561282bb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eO0MAmqiCQyl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}